# Variance partitioning in models: why the simple idea of overlapping variance is misleading

<section markdown="1">
We often come across presentations and publications that use the idea of variance partitioning to quantify how much of the variance in the measured brain responses can be explained by a certain experimental feature. In this blog, we point out that the uncritical application of the this idea can lead to the wrong intuitions and misleading conclusions.

The idea of variance partitioning in linear models goes back to Fisher's ANOVA [+citep:fisher_statistical_1925]. Here it is indeed the case that we can decompose the variance of a variable $Y$ into a part that can be explained by predictor $X_1$, a part that can be explained by predictor $X_2$, and a portion that is unexplained by either (Fig. 1a). The proportion of variance explained by a certain set of predictors is the $R^2$ value for the model. In the case of a balanced ANOVA, where $X_1$ and $X_2$ are orthogonal, the amount of variance explained when we combine the two regressors into a joint model ($R_{1 \cup 2}^2$) is the sum of the amount of variance explained by each one alone ($R^2_1 + R^2_2$). Thus, in this we can think about the variance of $Y$ as a pie that can be sliced into different parts.

Some authors in neuroscience have generalized this intuition to the more general case in which $X_1$ and $X_2$ are correlated [+citep:heer_hierarchical_2017]. In these cases, most often the variance explained by two predictors together is smaller than the sum of the variance explained by each regressor alone. This is because, so the logic goes, there is a "shared" proportion of the variance that can be explained by either regressor, that is $R_{1 \cup 2}^2 = R_{1}^2  + R_{2}^2 - R^2_{1 \cap 2}$. Often the different proportions are represented as a overlapping Venn diagram (Fig 1b). Following this logic, the variance explained by one regressor alone ($R^2_1$) consist of the 'shared' variance ($R^2_{1 \cap 2}$) and the part that is 'uniquely' explained by the regressor ($R^2_{1 \\ 2}$). This approach can be used for more than 2 groups of regressors.

![Figure 1](venn_diagram.jpg)**Figure 1.** Depicting explained variances as a Venn diagram. (a) For independent regressors, the variance explained by each regressor adds to the variance explained by both regressors together ($R_{1 \cup 2}^2 = R^2_{1}  + R^2_{2}$). (b) For correlated regressors the variance explained by both regressors is though of as the sum of the individual variances minus a portion that can be explained by both regressors, the shared portion of the variance ($R^2_{1 \cap 2}$).

So far, so intuitive. However, thinking about the variance explained by different models as a Venn-diagram can give you a number of wrong intuitions - and cause a number of apparent paradoxes that will seem very confusing. In this blog we want to illustrate some of these cases and provide some more helpful visual intuitions.

The first incorrect intuition you may take away from Fig 1b is that the variance explained by two regressor together ($R^2_{1 \cup 2}$) can never be bigger that the sum of the variances explained by each regressor alone. After all, that would make the shared variance ($R^2_{Y|1V \cup 2}$) negative, right? Unfortunately, it is quite easy to generate a situation in which $R_{1 \cup 2}^2$ is larger than the sum of $R_{1}^2$ and $R_{2}^2$. This phenomenon is known is the statistical literature as suppression.

To understand when and how suppression occurs, let us consider the space of all possible correlations between two regressors ($r_{1,2}$), and between the dependent variance and each regressors  ($r_{y,1}, r_{y,2}$). Knowing these three correlations is enough to analytically derive the different explained variances for the simple 2-regressor case (see Appendix). Not all combinations of these three correlations are possible - for example, if the two regressors are highly positively correlated, then their correlation with the dependent variable must be either both positive or both negative. The space of of possible 3x3 correlation forms the geometric shape depicted in Figure 2.

![Figure 2](surface_plot.jpg)**Figure 2.** Space of possible combinations of correlations of the two regressors ($r_{1,2}$) and the correlation of the dependent variable with either of the two regressors ($r_{y,1}, r_{y,2}$). For the blue areas, $R_{1 \cup 2}^2$ is smaller than $R_{1}^2 + R_{2}^2 $, in the red areas $R_{1 \cup 2}^2$ is larger than $R_{1}^2 + R_{2}^2$. The arrows refer to the approximate locations depicted in Figure 3.

Along the 'equator' of this shape, where the two regressors are uncorrelated, the explained variance of the joint model is the sum of the individual explained variance, indicated by the white color.  A very useful graphical intuition is to think about the two regressors and the data as vectors in a $N$-dimensional space, with $N$ being the number of observations in our data vector (Figure 3). Simple regression can then be thought of as the projection of the data ($\mathbf{y}$) onto the vector ($\mathbf{x}_1$), or the vector ($\mathbf{x}_2$). For the joint model (multiple regression) the projection is onto the plane spanned by both vectors. The length of the vector of the predicted data ($\hat{\mathbf{y}}$) can be understood of the size of the predicted value - and the squared length of the vector is the explained variance of the model.

![Figure 3](projections.jpg)**Figure 3.** Linear regression viewed as geometric projection of the data vector ($mathbf{y}$) onto the regression vector $mathbf{x}_1$, $mathbf{x}_2$, or the plane spanned by both. The predicted value ($\hat{\mathbf{y}}$) for each of the model is shown as a green dots. The amount of explained variance for each model is the squared distance of this dot from the origin. The residuals for each model are indicated by blue dashed lines.

In the case of orthogonal regressors (Fig. 3a), we can immediately see (from the Pythagoream theorem $c^2 = a^2 + b^2$ that $R^2_{1 \cup 2} = R_{1}^2 + R_{2}^2$. For correlated regressors, the situation is a bit more complicated. When the predicted value $\hat{\mathbf{y}}$ falls right between the two regressors (Fig 3b), the squared length of the predicted vector (green line) is substantially smaller than the sum of the original prediction.

In contrast, Fig. 3c shows a situation in which $X_1$ by itself explains very little of the data, However, when entering in into a joint model with $X_2$, it makes the prediction for the model substantially better, leading to a situation in which $R^2_{1 \ cup 2} > R^2_1 + R^2_2$. The phenomenon is called suppression, because we can think about $X_1$ suppressing or removing parts of $X_2$ that does not help in the prediction of Y, without explaining $Y$ directly. As can be seen from Fig. 2, this effect dominates for half of the possible values of correlations.

Ok, you may say, but as long as we are in a situation as in Fig. 3b, there $R_{1 \ cup 2}^2 < R^2_1 + R^2_2$, variance partitioning should work fine? After all, suppression is not observed very often in practice.{+side:This is likely because most often both regressors are positively correlated with each other and the data.} Unfortunately, however, suppression effect occur all the time. The difference between the $R^2$ of the joint model and the sum of $R^2$ of the single model is always determined both the balance between the shared variance and suppression, even if suppression effects do not dominate.

This can lead to another incorrect intuition: when we observe that the $R^2$ of the joint model equals the sum of $R^2$s of the single model, using variance partitioning, we would conclude that the shared variance is zeros and the two regressor are explaining independent aspects of the data. As we can see in Fig 2, this is not the case. Even for cases with very high correlations between regressors, there are situations in which the shared variance and suppression effects cancle each other out (white areas). In this case, the predicted values of the single model are also correlated - so the two regressors DO explain some overlapping aspects of the data.

In conclusion, the estimates of shared variance from variance partition is most of underestimating the true overlap between models - and therefore will overestimate the unique contribution. Even if

</section>
