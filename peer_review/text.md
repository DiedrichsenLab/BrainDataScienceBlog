#What would happen if we graded instead of rejecting paper?
# Rethinking peer review: Can we change what journals do? 

# Rating instead of rejecting papers

<section markdown="1">
*This blog is inspired by ongoing dicussions in the eLife  editorial working groups, especially with Michaela Iordanova, Tim Behrens, and Andy Collings. That being said, the ideas and opinions expressed here do not reflect a consensus on the eLife editorial board, but purely reflect a single opinion. In short - good idea are likely do to conversation with the community of eLife editors, the bad ideas are all mine.*
</section>
<section markdown="1">


## On the current state of peer review
First off, I am a big believer in peer review. Having an obligatory, primary, and structured peer review of every single paper is an important assurance to readers of scientific literature, and most often provides authors with useful feedback for making the paper clearer before it reaches it final form. After all, peer reviewers are usually among the few readers that will really feel motivated  to read the paper in detail and really understand it.{+side: Thus I do not think that replacing primary peer review completely with a voluntary and unregulated post-publication review [+citep: Fisher2015] would realistically work. Too many papers would not be assessed well enough or not assessed at all.} In sum, peer review is a little bit like democracy. There are a lot of things wrong with the system, but in among all possible alternative systems, it is likely the least unfair and malevolent. So before we starting to think about how to improve peer review, it is important to consider the functions that the current system is fulfilling. In my opinion, these are two-fold: 

The first function of peer review, revision, and possibly re-review is to make a scientific paper better. I have experienced many cases - both as an editor, reviewer, and as an author -  which a paper substantially changed and improved through a constructive dialogue between all parties involved. I know - it doesn't always work, but most of the time, reviewer comments can alert authors to possible gaps, mistakes, omissions, and blind spots. Even a reviewer that dramatically misunderstands my paper, provides the chance to figure make how to make the paper clearer to avoid subsequent misunderstandings. After all, our future readers won't be systematically different from your reviewers (except that they may give up earlier). 

The second function of peer review, is to provide a quality evaluation of papers.  We are all looking for fast and easy indicators of which papers are worth the time to read them. Currently, one of these indicators is the journal name. It serves - for the better or the worse - as a proxy for the quality of a paper, a whole lab, or the quality of a job candidate.  We have come to expect that a *Nature Neuroscience* paper should be better than a *PLoS One* paper. However, which journal a paper is published in, is  a product of a series of submit and reject decision, each of which is highly susceptible to social influences and subjectivity.  

Problems with peer review arises often from the fact that these two functions are often incompatible with each other. Reviewers often wish to signal that the paper is "not good enough" for the specific journal, and this will overshadow their whole review, making their comments less than helpful for improving the paper. Most reject decisions do not signal that the paper a deep flaw with a paper, often they simply mean that the paper wasn't judged to be of broad enough interest. To game the system, authors overstate the importance of their work, consciously not citing previous literature to stress its novelty and uniqueness. This obfuscates the literature, and the repeated review in a number of journals make the system slow and inefficient. 

One of the stated long-term goals of eLife is therefore to "move science away from the use of journal titles as the primary measure of the quality of research"  https://elifesciences.org/articles/64910. 

Of course, it would be ideal if we could make everybody **read** the papers that they are judging. Expecting a job search committee to read most of the papers from most of the candidates would be simply illusionary. Thus, if we like it or not, as a scientific community with bounded rationality, we are relying on quick heuristic to determine the quality of the paper. If we are taking away the journal name as one of the indicators without providing an alternative, the community will start using something else, such as the name of the Institution, number of social media engegaments, or other metrics that are more fraught with social bias than the current peer review system. So the best w can hope for is to create a system that is as accountable, transparent and fair as possible.      

## Alternative approach: 1-6/A-F Rating 

So here is one proposal for a simple rating scheme that *could* potentially be applied to all published empirical papers on pre-publication servers. The proposed rating occurs on two dimensions: the *Technical / Data Quality* and the *Importance of the scientific claim*. These two dimensions should be judged separately, as papers clearly differ in the way that they succeed in either category. The increase in sophistication of data collection techniques has led to labs specialising in creating high-quality data sets, and other labs to working on theoretical models and ideas. The rating system acknowledges that both types of papers are important for driving science forward. 

**The rating**

|-|**Importance of claim / insight**|-|**Technical Quality**|% of papers|
|-|----------------------------------------|-|----------------------------------------|---|
|A| The insights (if true) will substantially change the way we think about an entire field - or have life-changing practical implications. Important idea that everybody in the field should know about. |1| The data and analysis exceed by far the current state of the art in the field. A major methodological step forward that will shape research methodology in the field for years to come. |2%|
|B |The insights are novel and change an important aspect of a current theory or research question. |2| Data and analysis are of very high quality, in multiple aspects more rigorous or inventive than the current state of the art. |8%| 
|C|The insights provide an important piece of knowledge that speaks to an important research question. Is likely of interest to readers outside of the specific subfield.|3|Data and Analysis are performed according to state of the art, solidly supporting the conclusions. No real weaknesses. |15%|
|D|The insights have substantial theoretical or practical implications for a specific field, but are mainly interesting to readers in that subfield. |4| Data and Analysis are generally conclusive, only with minor weaknesses. |40%|
|E|Useful, but with limited importance or scope. Replication of well-known phenomena or relatively incremental advance.|5|Data and Analysis are of sufficient quality to provide some support for claims. Due to some weaknesses in experimental design, data set, and/or analysis, there are some limitations in the interpretation. |35%|
|F|Ill-posed scientific question or ignores important previous results. Should not be published in any journal until revised. |6|Insufficient quality of data or analysis to support the claim. Should not be published in any journal until revised. |-|

With these two rating scales, very few papers (<1%) will receive an A1 rating. A highly elegant technical contribution that is mostly interesting to a restricted field may receive an D1 rating. In contrast, a paper that presents a very interesting idea (aka the “sexy” story), but only provides some support for the claims, could be labelled as A5. Papers of insufficient technical quality, or with ill-posed research questions, will receive a F or 6 mark, respectively. The function of any peer review is to not let these papers become part of the scientific record without major revisions. Revisions would either need to improve data or methods or tone down the claims of the paper. Thus, these categories would not be chosen for papers that are accepted by any journal. The rating of the paper should be accompanied by a written evaluation summary and the public review that justify the rationale behind the final rating. 

![Figure 1](evaluation_rating.png)**Figure 1)** Two-dimensional rating of all papers. A rough estimation of the papers that are currently being reviewed in eLife (light blue), and that are being judged as having sufficient quality after revision (darker blue). Note that some categories with extreme discrepancy (A4, A5) are rather rare. It is likely that some papers currently published eLife papers would be rated with C3. 

### Proposed process 
{{Review process:}} It is important that the rating of papers does not interfere with a constructive peer-review process that is aimed at improving the paper. Our recommendation would therefore be that ratings and written evaluation summaries and ratings are only attached to the final accepted version, or that b) if they are attached to rejected papers, or to papers after the first round of review, that they are clearly visible as just referring to the current version on the pre-publication server. We think the rating scheme, however, can be useful as a **communication tool** in the review process. The editorial screening process before review can be made more objective and we can track and document why papers are not being sent out to review. A editorial assessment could read: “In the current version, I believe that some of the most interesting and highly controversial claims are not fully supported by the evidence (A6). If the authors concentrate on the interesting topic of X, I believe that the paper has the potential to become an important and solid contribution to the literature (B3) and should therefore be reviewed”. 

{{At final decision:}} When the reviewers and editor ultimately decide that the paper has its final form, the handling editor will draft the evaluation summary + the rating. We believe that the rating will improve the quality and impact of the evaluation summary - the written summary will need to spell out the aspects that determine the importance of insight and technical quality ratings, rather than just summarising the main findings. Thus we believe that a rating scheme will help the evaluation summaries to become more concrete and incisive. The editor will send both rating and evaluation summary to the reviewers for discussion and approval. Finally, both are being reviewed and approved by the senior editor. If there is disagreement among reviewers and handling editors, the senior editor should go back and consult with the BREs that were involved in the initial screening of the paper, until a final rating is achieved. In case of exceptionally highly rated papers, the senior editor may want to consult even with a broader range of senior and board editors to ensure that the paper really is of broad interest as the rating indicates.  The decision letter will have the final evaluation summary and rating. 

### Introducing the system 
The first (not unrealistic) step is not all that unrealsitic is to apply the rating to all published papers in eLife together with the evaluation summary. The rating and evaluation summary should be clearly visible on the published paper, and ideally accessible on pubmed and preprint servers. This is suplemented The reviews and responses of the authors 

One immediate positive consequence would be that eLife could be less selective with accepting papers and reviewing papers, without risking the to inflating the value of the journal name. That is, a A1 paper in eLife will still have the same value, no matter how many D4 papers maybe published alongside it in the same issue. The need for editorial screening before review would be dramatically reduced and would only be limited by the number of available handling editors and reviewers. 

The next step is to convince other journals to join a consortium that applies the judgement scale consistently. Specialized journals (or so called "lower-tier" journals) may publish predominatly E5 papers. To allow for some dsicrimination and the abilty highlight relative strength of some papers, it maybe useful to introduce subcategories, such as E- or E+ ratings. 

## What could possibly go wrong? 
### Authors will want to appeal the final rating
We anticipate that (as currently with paper rejections), many authors will want to appeal the final rating. To manage this process, we think that it is useful to adopt the following two techniques: At the first revise-resubmit decision, the editor should ideally outline the potential for the rating. I.e., a decision letter could read: *If you can clarify the remaining issues, the reviewers think that the final evaluation will be a B3. For a higher rating of technical quality, we think that new data collection addressing problem X would be necessary.* If we can make this common practice, control over the fate of the paper would be handed back to the authors: They can address the problem with new data collection and analysis if they agree that this is worth it. They can op to revise and publish the paper without new data collection to get the paper out. Or they can walk and take the paper elsewhere if they are unhappy with the evaluation. Either way, this transparencey at early stages will leikly be helpful to cap off many of the appeals. 

### It places too much power in the hand of editors and reviewers
Many of us may feel uncomfortable with being judge in such a public and quantifyable way. After all, editors and reviewers are just a very small sample of future potential readers. Is it not handing a small handful of people the ability to make or break scientific careers?

These concerns valid and real. The fact, however, is that this power is already lying with editors and reviewers who ultimately make the reject /accept decision. The power for the highest ratings lies with professional editors of journals like *Nature* and *Science*, and the reviewers that they select, and motivations and criteria are not accountable or transparent. So how can a new system improve the system and make it less arbitrary or biased? I believe that the following three steps would help addressing some of these concerns. 

Fist, at the final decision point, the authors should be given the option of withdrawing the paper in its current form and not publish with the suggested. The open reviews and rating would be then attached to the submitted version on the preprint server. However, the authors ahve the option to revise the paper which would be uploaded and seek review and possibly rating by a different journal. 

Second, the primary peer reviews and initial rating should be only the starting point for a continued evlaiution of the paper in the scientific record. Thus, a platform for secondary post-publication review, and the ability for individuals other review consortia (such as F1000) to propose and justify competing ratings of a paper would help to take the edge off the stochasticity of the initial rating. Havng a pularity of opinions out there will hopefully help to establish the rating system as a helpful heuristics and communication tool, rather than a final judgement.  

Third, we require a mechanism by which  editorial boards would be able to revise the initial rating of papers. Often the real impact of papers is becoming clearer a few years after publication. A new proposed methodology or theory may have become widely adopted, methologocal faults in a paper may be subsequently revealed, or replications of a proposed effect may systematically fail. While revisions of ratings will likely be the exception than the rule, it hopefully will encourage authors to focus on producing lasting contributions to the literature, rather than aiming for a quick, but transient impact. 

### Authors will go shopping for better ratings and journals will serve them
A flip-side of allowing authors to submit a evaluated paper elsewhere is that the new system would just replicate exactly the same problems that make the current system inefficient: The time of reviewers, editors, and scientists would be wasted. However,  forsaking a quick publication of a paper for a future, but highly uncertain, better rating, should hopefully not be too attractive to authors. Also, having older reviews of previous versions of papers available as part of the publica record would help mitgate the probel. It would allow authors to fully respond to the concerns raised in the reviews and ask for a alternate opinion, but would prevent a complete replication of the process. 

A related problem is that specific journals (or review consortia / journal groups) may start to inflate ratings to attract both authors and readers. How to ensure consistency of the rating across journals or review consortia is a thorny issue (see introduction of the system). A centralized oversight board that enforces the standards and in the worst case withdraws the permission to journals to use the rating system would be one solution. Ideally, however, the system should be self-correcting without the need for a central authority. One possible mechanism is to publish function relating the  primary rating to the externally procured secondary ratings of the same papers - and making the goal of each journal/consortium to keep this curve close to the indentiy line. In the worst-case, a correction factor would need to be applied to the ratings from journals and journal groups that use the rating system in an inflationary way.   

## Conclusions

</section>

<section markdown="1">

## Acknowledgements

</section>